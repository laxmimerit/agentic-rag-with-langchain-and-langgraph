{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hide warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "FILE_PATH = r\"..\\00 Dataset\\docs\\facebook\\Meta-09-30-2024-Exhibit-99-1_FINAL.pdf\"\n",
    "\n",
    "loader = DoclingLoader(file_path=FILE_PATH,\n",
    "                       export_type=ExportType.MARKDOWN,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Meta-09-30-2024-Exhibit-99-1_FINAL.pdf\n",
      "INFO:docling.document_converter:Finished converting document Meta-09-30-2024-Exhibit-99-1_FINAL.pdf in 10.88 sec.\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content)\n",
    "# write markdown to file\n",
    "with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(docs[0].page_content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to MarkDown with Images and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Earnings-Presentation-Q3-2024.pdf\n",
      "INFO:docling.document_converter:Finished converting document Earnings-Presentation-Q3-2024.pdf in 34.33 sec.\n",
      "INFO:__main__:Document converted and figures exported in 43.36 seconds.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path('localfile path')\n",
    "    input_doc_path = r\"..\\00 Dataset\\docs\\facebook\\Earnings-Presentation-Q3-2024.pdf\"\n",
    "    output_dir = Path(\"scratch\")\n",
    "\n",
    "    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n",
    "    # will destroy them for cleaning up memory.\n",
    "    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n",
    "    # scale=1 correspond of a standard 72 DPI image\n",
    "    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n",
    "    # with the image field\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem\n",
    "\n",
    "    # Save page images\n",
    "    for page_no, page in conv_res.document.pages.items():\n",
    "        page_no = page.page_no\n",
    "        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "        with page_image_filename.open(\"wb\") as fp:\n",
    "            page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "    # Save images of figures and tables\n",
    "    table_counter = 0\n",
    "    picture_counter = 0\n",
    "    for element, _level in conv_res.document.iterate_items():\n",
    "        if isinstance(element, TableItem):\n",
    "            table_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "        if isinstance(element, PictureItem):\n",
    "            picture_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "    # Save markdown with externally referenced pictures\n",
    "    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Image with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image presents a comprehensive overview of financial data across four distinct categories: Rest of World, Asia-Pacific, Europe, and US & Canada. Each category is further divided into six quarters, spanning from Q1'23 to Q3'24.\n",
      "\n",
      "**Category Breakdown**\n",
      "\n",
      "*   **Rest of World**\n",
      "    *   Q1'23: $27,714\n",
      "    *   Q2'23: $31,999\n",
      "    *   Q3'23: $32,165\n",
      "    *   Q4'22: $28,645\n",
      "    *   Q1'24: $39,071\n",
      "    *   Q2'24: $40,589\n",
      "*   **Asia-Pacific**\n",
      "    *   Q1'23: $5,782\n",
      "    *   Q2'23: $6,515\n",
      "    *   Q3'23: $7,050\n",
      "    *   Q4'22: $5,960\n",
      "    *   Q1'24: $8,483\n",
      "    *   Q2'24: $9,492\n",
      "*   **Europe**\n",
      "    *   Q1'23: $13,035\n",
      "    *   Q2'23: $14,422\n",
      "    *   Q3'23: $15,636\n",
      "    *   Q4'22: $13,048\n",
      "    *   Q1'24: $16,847\n",
      "    *   Q2'24: $17,609\n",
      "*   **US & Canada**\n",
      "    *   Q1'23: $5,100\n",
      "    *   Q2'23: $6,515\n",
      "    *   Q3'23: $7,050\n",
      "    *   Q4'22: $5,960\n",
      "    *   Q1'24: $8,483\n",
      "    *   Q2'24: $9,492\n",
      "\n",
      "In conclusion, the image provides a detailed breakdown of financial data across four categories and six quarters. The data highlights fluctuations in each category over time, with some showing growth while others exhibit decline.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import base64\n",
    "\n",
    "# read image from file\n",
    "with open(r\"scratch\\Earnings-Presentation-Q3-2024-with-image-refs_artifacts\\image_000004_8139d1246423312e74e78335d99a132a935f7de1b62a31df96dc7d8c91a47a7a.png\", \"rb\") as f:\n",
    "    image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "model = ChatOllama(model=\"llama3.2-vision\", base_url=\"http://localhost:11434\")\n",
    "# model = ChatOllama(model=\"llama3.2\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "\n",
    "system = SystemMessage(\"\"\"\n",
    "                Extract detailed financial information from the provided image. Start by identifying the company name, document title, and any relevant details from the header and footer.\n",
    "\n",
    "                Ensure to:\n",
    "                    - Thoroughly extract all financial figures and metrics mentioned, such as revenue, profit, assets, liabilities, etc.\n",
    "                    - Explain the financial data with technical details, including any relevant financial terminology or calculation methods.\n",
    "                    - Summarize any regulatory or legal information provided in the document.\n",
    "                \n",
    "                Provide a complete and detailed description of the image, ensuring no important data is missed.\n",
    "                       \"\"\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Extract financial information from the image below.\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarkDown Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "markdown_path = r\"scratch\\Earnings-Presentation-Q3-2024-with-image-refs.md\"\n",
    "with open(markdown_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_urls_and_clean_content(page_content):\n",
    "    # Define the regex pattern to match image URLs\n",
    "    pattern = r\"!\\[Image\\]\\(([^)]+)\\)\"\n",
    "\n",
    "    # Find all matches in the page content\n",
    "    image_urls = re.findall(pattern, page_content)\n",
    "\n",
    "    # Remove all matched image URLs from the content\n",
    "    cleaned_content = re.sub(pattern, \"\", page_content)\n",
    "\n",
    "    return image_urls, cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000000_3db7e1eab3213fbdaaf83be4c917fbb9fd97bdf42a5033bd7e7e4df4b53f7b18.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000001_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000002_1251d14efb5cba3ebd6d19703f740b8b0bb6b1d1a99275c9eb52941cb5e4c0f1.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000003_7a5af1bd15760003c9cd20a1f0ac84b7eb0585e9c23c8d401c7e4a3226887e25.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000004_8139d1246423312e74e78335d99a132a935f7de1b62a31df96dc7d8c91a47a7a.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000005_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000006_dee327b9119ef6ee2940999dc13e68a8c93145e5d96b82e0461dfa7b71598c73.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000007_f487c223a58e4579f4214fd6028bfdfd73096cca22887583167134ed5a1da0fe.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000008_dee327b9119ef6ee2940999dc13e68a8c93145e5d96b82e0461dfa7b71598c73.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000009_dd3b977898773b0827f2efff8a8e345742694ca4ec3fd89d11727db93af2cd68.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000010_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000011_b1573019cc9d988c8048d15bc415712c40691dbc5236eb065e274de5a4eefa64.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000012_3dbd6ce9176178bbe19bdce419dec3d6aa2c7bdcc97a33ceca26515d3c6317bf.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000013_f2250e6474c3402323412b96a68692489027330905f02ff73358bf5ba9245edf.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000014_acce4c5fc0db73d79d4eca75003e14ee4c856baebaffa7643561c92a63f25e1a.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000015_9a0d8c8c11689e3ac39e45f2d0c4a20c2ac647470427edfc3084e927a4056395.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000016_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000017_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000018_672aac4260b0e4739f7bbf750987785000864671d723074cc79ad673dd632559.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000019_e23ca26fa1ccabb8a87b15ced17639aaaa393a4af4282bbd21b17d25d5d2522f.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000020_90f96ad7cd25f09f449894371119746fd114c216e0ac8b884a022d1bf2bca358.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000021_483bea5766950fd3817857815179cc391853411b3ec1ed5df41c370718de2c4f.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000022_1f3033dac443fe1a2dc1454e427aa6332d0df9e080f30dfd0af9d9b59a6d7162.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000023_d73828ca641ef88134088622c449de0debc3168d484003fb5d7248b6d1ad0a85.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000024_dee327b9119ef6ee2940999dc13e68a8c93145e5d96b82e0461dfa7b71598c73.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000025_4b16af319e9393ae70ded10db6fa9af9d56fcdb4df90690d67f6b0fa9aec1f0f.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000026_1dbe70b1573309e99360644152ddc7c3c230a23b4f4cb7d23f1f5f5d8fc06a4a.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000027_983d760274173d3a8c2c0c4214afe4a4ce0d2e501e88cb592788b4dacfe43d92.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000028_283ddd687bd4542207aa71bb389985399fda059475ba7a8cf2512bf50d6592a1.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000029_fb82481a2250711d2fdb303279a2343584a2b9b2e6b8d975f9a037d1603ff1d9.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000030_bfc82afd96c94336e0fe98bc6950ac5a226e0826ca3f4ce488897cda70b65b6f.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000031_61331cf29cbe41e0cfd7585af316c8f4a55b1ab6ef528e0d2dfc9640cb8022a5.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000032_fc1e32a9faebdcec75dc7acb7f32b5bc5fafd2b6681eac5f0e43abad585bd063.png']\n",
      "Image URLs: []\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000033_34a3fafa6be647cb65ed649fc2257054f06cba213a04887be094897b444badcf.png']\n",
      "Image URLs: []\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000034_34a3fafa6be647cb65ed649fc2257054f06cba213a04887be094897b444badcf.png']\n",
      "Image URLs: []\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000035_1d0090d2a602b18b054d6d31be46f4fa66c6333f8b3e4ce0a1f93de3d7231ec8.png']\n"
     ]
    }
   ],
   "source": [
    "for page in md_header_splits:\n",
    "    # print(f\"Header: {page.metadata}\")\n",
    "    # print(f\"Content: {page.page_content}\")\n",
    "    # Extract image URLs from the page content\n",
    "    image_urls, cleaned_content = extract_image_urls_and_clean_content(page.page_content)\n",
    "    print(f\"Image URLs: {image_urls}\")\n",
    "    # print(f\"Cleaned Content: {cleaned_content}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich MarkDown with Images Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:  {'Header 2': 'Meta Earnings Presentation Q3 2024'}\n",
      "response:  The image presents a financial report from Meta, detailing its quarterly earnings for the third quarter (Q3) of 2024.\n",
      "\n",
      "**Header Section**\n",
      "\n",
      "*   The top section features the company's logo on the left side, accompanied by the title \"Meta\" in large font.\n",
      "*   Below the title, the text \"Earnings Presentation Q3 2024\" is displayed.\n",
      "\n",
      "**Financial Data**\n",
      "\n",
      "The report provides detailed financial information for the third quarter of 2024, including:\n",
      "\n",
      "*   **Revenue**: $34.4 billion\n",
      "    *   Represented as a bar graph with a blue line indicating growth.\n",
      "    *   A slight decline in revenue compared to Q3 2023 ($35.8 billion).\n",
      "*   **Net Income**: $9.2 billion\n",
      "    *   Shown as a bar graph with a red line indicating a decrease.\n",
      "    *   Decrease of approximately $1.6 billion compared to Q3 2023 ($10.8 billion).\n",
      "*   **Operating Expenses**: $15.7 billion (up from $14.4 billion in Q3 2023)\n",
      "    *   Represented as a bar graph with a green line indicating growth.\n",
      "*   **Research and Development (R&D) Expenses**: $5.6 billion\n",
      "    *   Shown as a bar graph with a yellow line indicating an increase.\n",
      "*   **Capital Expenditures**: $2.4 billion\n",
      "    *   Represented as a bar graph with a purple line indicating growth.\n",
      "\n",
      "**Other Sections**\n",
      "\n",
      "The report also includes sections on:\n",
      "\n",
      "*   **Segment Performance**: Breakdown of revenue by segment (e.g., Facebook, Instagram, WhatsApp).\n",
      "*   **Geographic Revenue**: Distribution of revenue across different regions.\n",
      "*   **Key Metrics**: Overview of key performance indicators (KPIs) such as user growth, engagement, and monetization.\n",
      "\n",
      "**Footer Section**\n",
      "\n",
      "The footer section contains:\n",
      "\n",
      "*   A disclaimer stating that the information is unaudited and subject to change.\n",
      "*   A note indicating that the report will be updated with audited financial statements in due course.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "vision model only supports a single image per message",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 53\u001b[0m\n\u001b[0;32m     47\u001b[0m text_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}\n\u001b[0;32m     49\u001b[0m message \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[0;32m     50\u001b[0m     content\u001b[38;5;241m=\u001b[39m[text_message] \u001b[38;5;241m+\u001b[39m images_data,\n\u001b[0;32m     51\u001b[0m )               \n\u001b[1;32m---> 53\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke([system_message, message])\n\u001b[0;32m     55\u001b[0m merged_content \u001b[38;5;241m=\u001b[39m cleaned_content \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracted Image Description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     57\u001b[0m documents\u001b[38;5;241m.\u001b[39mappend(merged_content)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[0;32m    645\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    646\u001b[0m     )\n\u001b[0;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[0;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[0;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[0;32m    655\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    547\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[0;32m    548\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[0;32m    549\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    562\u001b[0m                 ),\n\u001b[0;32m    563\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m    530\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    531\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m    532\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    534\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\ollama\\_client.py:85\u001b[0m, in \u001b[0;36mClient._stream\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     84\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m     88\u001b[0m   partial \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[1;31mResponseError\u001b[0m: vision model only supports a single image per message"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "import base64\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "model = ChatOllama(model=\"llama3.2-vision\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "\n",
    "system_message = SystemMessage(\"\"\"\n",
    "                Extract detailed financial information from the provided image. Start by identifying the company name, document title, and any relevant details from the header and footer.\n",
    "\n",
    "                Ensure to:\n",
    "                    - Thoroughly extract all financial figures and metrics mentioned, such as revenue, profit, assets, liabilities, etc.\n",
    "                    - Explain the financial data with technical details, including any relevant financial terminology or calculation methods.\n",
    "                    - Summarize any regulatory or legal information provided in the document.\n",
    "                \n",
    "                Provide a complete and detailed description of the image, ensuring no important data is missed.\n",
    "                       \"\"\")\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for page in md_header_splits:\n",
    "    image_urls, cleaned_content = extract_image_urls_and_clean_content(page.page_content)\n",
    "    # read image from file\n",
    "    images_data = []\n",
    "    for url in image_urls:\n",
    "        url = f\"scratch/{url}\".replace(\"%5C\", \"/\")\n",
    "        with open(url, \"rb\") as f:\n",
    "            img_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "            # {\"type\": \"image_url\",\n",
    "            # \"image_url\": {\"url\": f\"data:image/png;base64,{image_data}\"}}\n",
    "\n",
    "            images_data.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/{url.split('.')[-1]};base64,{img_base64}\"}})\n",
    "\n",
    "    text = f\"\"\"Here is some reference content for the image. You need to ensure the generated image description fits into the given context.\n",
    "                Do not write any preamble or explanation other than asked in the task described.\n",
    "\n",
    "                ### Content to Get The Idea What This Image Is About:\n",
    "                {cleaned_content}\n",
    "\n",
    "                Generate a detailed description of the image. \n",
    "                Ensure that the description is comprehensive and no important data is missed.\n",
    "                ### Image Description:\"\"\"\n",
    "    \n",
    "    text_message = {\"type\": \"text\", \"text\": text}\n",
    "    \n",
    "    message = HumanMessage(content=[text_message] + images_data)      \n",
    "\n",
    "    # ResponseError: vision model only supports a single image per message\n",
    "    message = HumanMessage(content=[text_message, images_data[0]])               \n",
    "    \n",
    "    response = model.invoke([system_message, message])\n",
    "    \n",
    "    merged_content = cleaned_content + \"\\n\\nExtracted Image Description:\\n\" + response.content\n",
    "\n",
    "    documents.append(merged_content)\n",
    "\n",
    "    print(\"page: \", page.metadata)\n",
    "    print(\"response: \", response.content)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "enriched_content = \"\\n\\n\".join(documents)\n",
    "\n",
    "with open(\"enriched_content.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(enriched_content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
