{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Doclings - PDF (Any Documents) to MarkDown with Images and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich MarkDown with Images Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## hide warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "import base64\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# model = ChatOllama(model=\"llama3.2-vision\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "system_message = SystemMessage(\"\"\"\n",
    "                Extract detailed financial information from the provided image.\n",
    "                Start by identifying the company name, document title, and any relevant details from the header and footer.\n",
    "\n",
    "                Ensure to:\n",
    "                    - Thoroughly extract all financial figures and metrics mentioned, such as revenue, profit, assets, liabilities, etc.\n",
    "                    - Explain the financial data with technical details, including any relevant financial terminology or calculation methods.\n",
    "                    - Summarize any regulatory or legal information provided in the document.\n",
    "                \n",
    "                Provide a complete and detailed description of the image in the form of table if possible.\n",
    "                       \"\"\")\n",
    "\n",
    "def get_image_description(image_data_path, image_urls, cleaned_content):\n",
    "    images_data = []\n",
    "    for url in image_urls:\n",
    "        url = f\"{image_data_path}/{url}\".replace(\"%5C\", \"/\")\n",
    "        with open(url, \"rb\") as f:\n",
    "            img_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "            img_dict = {\"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/{url.split('.')[-1]};base64,{img_base64}\"}}\n",
    "            \n",
    "            images_data.append(img_dict)\n",
    "\n",
    "    text = f\"\"\"Here is some reference content for the image. You need to ensure the generated image description fits into the given context.\n",
    "                Do not write any preamble or explanation other than asked in the task described.\n",
    "\n",
    "                ### Content to Get The Idea What This Image Is About:\n",
    "                {cleaned_content}\n",
    "\n",
    "                Generate a detailed description of the image. \n",
    "                Ensure that the description is comprehensive and no important data is missed.\n",
    "                ### Image Description:\"\"\"\n",
    "    \n",
    "    text_message = {\"type\": \"text\", \"text\": text}\n",
    "    \n",
    "    final_message = [text_message] + images_data\n",
    "\n",
    "    message = HumanMessage(content=final_message)\n",
    "\n",
    "    # ResponseError: vision model only supports a single image per message\n",
    "    # in case of Ollama Model, LLAMA3.2 Vision\n",
    "    # message = HumanMessage(content=[text_message, images_data[0]])               \n",
    "    \n",
    "    response = model.invoke([system_message, message])\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Doclings - PDF to MarkDown with Images and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ds4sd.github.io/docling/examples/export_figures/\n",
    "# https://github.com/laxmimerit/agentic-rag-with-langchain-and-langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "Could not load the custom kernel for multi-scale deformable attention: Command '['where', 'cl']' returned non-zero exit status 1.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Earnings-Presentation-Q3-2024.pdf\n",
      "INFO:docling.document_converter:Finished converting document Earnings-Presentation-Q3-2024.pdf in 31.39 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\\Earnings-Presentation-Q3-2024-with-image-refs.md\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "def get_pdf_markdown(input_doc_path, target_dir):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(input_doc_path)  # Ensure it's a Path object\n",
    "    target_dir = Path(target_dir)  # Ensure it's a Path object\n",
    "    output_dir = Path(\"scratch\")  # Intermediate directory for storing images\n",
    "\n",
    "    # Configure the pipeline options\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    # Initialize the document converter\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert the input PDF document\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    # Ensure the output directories exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem\n",
    "\n",
    "    # Save page images\n",
    "    for page_no, page in conv_res.document.pages.items():\n",
    "        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "        with page_image_filename.open(\"wb\") as fp:\n",
    "            page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "    # Save images of figures and tables\n",
    "    table_counter = 0\n",
    "    picture_counter = 0\n",
    "    for element, _level in conv_res.document.iterate_items():\n",
    "        if isinstance(element, TableItem):\n",
    "            table_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "        if isinstance(element, PictureItem):\n",
    "            picture_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "    # Save markdown with externally referenced pictures\n",
    "    md_filename = target_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "    return md_filename\n",
    "\n",
    "# Usage example\n",
    "input_doc_path = r\"Earnings-Presentation-Q3-2024.pdf\"\n",
    "md_filename = get_pdf_markdown(input_doc_path, target_dir=\"output\")\n",
    "print(md_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarkDown Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('output/Earnings-Presentation-Q3-2024-with-image-refs.md')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "# md_filename = r\"output\\Earnings-Presentation-Q3-2024-with-image-refs.md\"\n",
    "\n",
    "def get_markdown_splits(md_filename):\n",
    "    with open(md_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        markdown_content = f.read()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_content)\n",
    "\n",
    "    return md_header_splits\n",
    "\n",
    "md_header_splits = get_markdown_splits(md_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Family Daily Active People (DAP)  \n",
      "In Billions  \n",
      "![Image](Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000015_b38533143e867d40a45b28680284201d641d46c933fd05b19a2f1b7b6933c0b4.png)  \n",
      "We define a daily active person (DAP) as a registered and logged-in user of Facebook, Instagram, Messenger, and/or WhatsApp (collectively, our \"Family\" of products) who visited at least one of these Family products through a mobile device application or using a web or mobile browser on a given day. The numbers for DAP do not include users on our other products unless they would otherwise qualify as DAP based on their other activities on our Family products.  \n",
      "We do not require people to use a common identifier or link their accounts to use multiple products in our Family, and therefore must seek to attribute multiple user accounts within and across products to individual people. Our calculations of DAP rely upon complex techniques, algorithms, and machine learning models that seek to estimate the underlying number of unique people using one or more of these products, including by matching user accounts within an individual product and across multiple products when we believe they are attributable to a single person, and counting such group of accounts as one person. As these techniques and models require significant judgment, are developed based on internal reviews of limited samples of user accounts, and are calibrated against user survey data, there is necessarily some margin of error in our estimates. For additional information, see \"Limitations of Key Metrics and Other Data\" located in the Appendix of this presentation. In the third quarter of 2022, we updated our Family metrics calculations to maintain calibration of our models against recent user survey data, and we estimate such update contributed an aggregate of approximately 30 million DAP to our reported worldwide DAP in September 2022. Beginning in the fourth quarter of 2023, our Family metrics no longer include Messenger Kids users.  \n",
      "![Image](Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000016_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png)\n"
     ]
    }
   ],
   "source": [
    "print(md_header_splits[9].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_urls_and_clean_content(page_content):\n",
    "    # Define the regex pattern to match image URLs\n",
    "    pattern = r\"!\\[Image\\]\\(([^)]+)\\)\"\n",
    "\n",
    "    # Find all matches in the page content\n",
    "    image_urls = re.findall(pattern, page_content)\n",
    "\n",
    "    # Remove all matched image URLs from the content\n",
    "    cleaned_content = re.sub(pattern, \"\", page_content)\n",
    "\n",
    "    return image_urls, cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000015_b38533143e867d40a45b28680284201d641d46c933fd05b19a2f1b7b6933c0b4.png',\n",
       "  'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000016_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png'],\n",
       " '## Family Daily Active People (DAP)  \\nIn Billions  \\n  \\nWe define a daily active person (DAP) as a registered and logged-in user of Facebook, Instagram, Messenger, and/or WhatsApp (collectively, our \"Family\" of products) who visited at least one of these Family products through a mobile device application or using a web or mobile browser on a given day. The numbers for DAP do not include users on our other products unless they would otherwise qualify as DAP based on their other activities on our Family products.  \\nWe do not require people to use a common identifier or link their accounts to use multiple products in our Family, and therefore must seek to attribute multiple user accounts within and across products to individual people. Our calculations of DAP rely upon complex techniques, algorithms, and machine learning models that seek to estimate the underlying number of unique people using one or more of these products, including by matching user accounts within an individual product and across multiple products when we believe they are attributable to a single person, and counting such group of accounts as one person. As these techniques and models require significant judgment, are developed based on internal reviews of limited samples of user accounts, and are calibrated against user survey data, there is necessarily some margin of error in our estimates. For additional information, see \"Limitations of Key Metrics and Other Data\" located in the Appendix of this presentation. In the third quarter of 2022, we updated our Family metrics calculations to maintain calibration of our models against recent user survey data, and we estimate such update contributed an aggregate of approximately 30 million DAP to our reported worldwide DAP in September 2022. Beginning in the fourth quarter of 2023, our Family metrics no longer include Messenger Kids users.  \\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_image_urls_and_clean_content(md_header_splits[9].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich MarkDown with Images Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def enrich_document_with_image(md_header_splits):\n",
    "    documents = []\n",
    "    for page in md_header_splits:\n",
    "        image_urls, cleaned_content = extract_image_urls_and_clean_content(page.page_content)\n",
    "        # read image from file\n",
    "        \n",
    "        image_data_path = \"output\"\n",
    "        image_description = get_image_description(image_data_path, image_urls, cleaned_content)\n",
    "            \n",
    "        \n",
    "        merged_content = cleaned_content + \"\\n\\nExtracted Image Description:\\n\" + image_description\n",
    "\n",
    "        documents.append(merged_content)\n",
    "\n",
    "        # print(\"page: \", page.metadata)\n",
    "        # print(\"response: \", image_description)\n",
    "        # print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "    enriched_content = \"\\n\\n\".join(documents)\n",
    "\n",
    "    return enriched_content\n",
    "\n",
    "enriched_content = enrich_document_with_image(md_header_splits)\n",
    "\n",
    "with open(\"output/enriched_content.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(enriched_content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Entire Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
