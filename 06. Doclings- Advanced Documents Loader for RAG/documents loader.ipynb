{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich MarkDown with Images Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## hide warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_openai.chat_models.azure import AzureChatOpenAI\n"
     ]
    },
    {
     "ename": "PydanticUserError",
     "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/custom-json-schema",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, SystemMessage\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_ollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_secret_str, get_from_dict_or_env\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[0;32m     42\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     45\u001b[0m _BM \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BM\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mBaseModel)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:297\u001b[0m\n\u001b[0;32m    293\u001b[0m     parsed: Optional[_DictOrPydantic]\n\u001b[0;32m    294\u001b[0m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseChatOpenAI\u001b[39;00m(BaseChatModel):\n\u001b[0;32m    298\u001b[0m     client: Any \u001b[38;5;241m=\u001b[39m Field(default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m#: :meta private:\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     async_client: Any \u001b[38;5;241m=\u001b[39m Field(default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m#: :meta private:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:224\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[0;32m    222\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[1;32m--> 224\u001b[0m complete_model_class(\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    226\u001b[0m     cls_name,\n\u001b[0;32m    227\u001b[0m     config_wrapper,\n\u001b[0;32m    228\u001b[0m     raise_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    229\u001b[0m     types_namespace\u001b[38;5;241m=\u001b[39mtypes_namespace,\n\u001b[0;32m    230\u001b[0m     create_model_module\u001b[38;5;241m=\u001b[39m_create_model_module,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_model_construction.py:577\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[1;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__get_pydantic_core_schema__(\u001b[38;5;28mcls\u001b[39m, handler)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:671\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[1;34m(cls, source, handler)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handler(source)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler(source_type)\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:655\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    652\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[0;32m    657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:924\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type_stack\u001b[38;5;241m.\u001b[39mpush(obj):\n\u001b[1;32m--> 924\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_schema(obj)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:739\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    727\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    729\u001b[0m         inner_schema,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    736\u001b[0m     )\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    738\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[1;32m--> 739\u001b[0m         {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_md_field_schema(k, v, decorators) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    740\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    741\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[0;32m    742\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    743\u001b[0m         ],\n\u001b[0;32m    744\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[0;32m    745\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    746\u001b[0m     )\n\u001b[0;32m    747\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    748\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1115\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1110\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1111\u001b[0m     field_info: FieldInfo,\n\u001b[0;32m   1112\u001b[0m     decorators: DecoratorInfos,\n\u001b[0;32m   1113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_field_schema(name, field_info, decorators)\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[0;32m   1117\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1118\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1123\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1308\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[1;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[0;32m   1305\u001b[0m             source_type, annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator\n\u001b[0;32m   1306\u001b[0m         )\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1308\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[0;32m   1309\u001b[0m             source_type,\n\u001b[0;32m   1310\u001b[0m             annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators,\n\u001b[0;32m   1311\u001b[0m         )\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2107\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[1;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[0;32m   2102\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[0;32m   2104\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[0;32m   2105\u001b[0m     )\n\u001b[1;32m-> 2107\u001b[0m schema \u001b[38;5;241m=\u001b[39m get_inner_schema(source_type)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[0;32m   2109\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[1;34m(self, source_type)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[1;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler(source_type)\n\u001b[0;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2088\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2086\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, source_type)\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2088\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2090\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:929\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[1;32m--> 929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_type(obj)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1029\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1027\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_generic_type(obj, origin)\n\u001b[0;32m   1031\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prepare_pydantic_annotations_for_known_type(obj, ())\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1058\u001b[0m, in \u001b[0;36mGenerateSchema._match_generic_type\u001b[1;34m(self, obj, origin)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_property\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39morigin_is_union(origin):\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_union_schema(obj)\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuple_schema(obj)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:1378\u001b[0m, in \u001b[0;36mGenerateSchema._union_schema\u001b[1;34m(self, union_type)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         nullable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         choices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_schema(arg))\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1381\u001b[0m     s \u001b[38;5;241m=\u001b[39m choices[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:657\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[1;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[1;32m--> 657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    659\u001b[0m     metadata_schema \u001b[38;5;241m=\u001b[39m resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefs\u001b[38;5;241m.\u001b[39mdefinitions)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2447\u001b[0m, in \u001b[0;36m_extract_get_pydantic_json_schema\u001b[1;34m(tp, schema)\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[0;32m   2446\u001b[0m         cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m   2448\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2449\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2450\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom-json-schema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2451\u001b[0m         )\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;66;03m# handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\u001b[39;00m\n\u001b[0;32m   2454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_annotated(tp):\n",
      "\u001b[1;31mPydanticUserError\u001b[0m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/custom-json-schema"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "import base64\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# model = ChatOllama(model=\"llama3.2-vision\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "system_message = SystemMessage(\"\"\"\n",
    "                Extract detailed financial information from the provided image. Start by identifying the company name, document title, and any relevant details from the header and footer.\n",
    "\n",
    "                Ensure to:\n",
    "                    - Thoroughly extract all financial figures and metrics mentioned, such as revenue, profit, assets, liabilities, etc.\n",
    "                    - Explain the financial data with technical details, including any relevant financial terminology or calculation methods.\n",
    "                    - Summarize any regulatory or legal information provided in the document.\n",
    "                \n",
    "                Provide a complete and detailed description of the image, ensuring no important data is missed.\n",
    "                       \"\"\")\n",
    "\n",
    "def get_image_description(image_data_path, image_urls, cleaned_content):\n",
    "    images_data = []\n",
    "    for url in image_urls:\n",
    "        url = f\"{image_data_path}/{url}\".replace(\"%5C\", \"/\")\n",
    "        with open(url, \"rb\") as f:\n",
    "            img_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "            img_dict = {\"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/{url.split('.')[-1]};base64,{img_base64}\"}}\n",
    "            \n",
    "            images_data.append(img_dict)\n",
    "\n",
    "    text = f\"\"\"Here is some reference content for the image. You need to ensure the generated image description fits into the given context.\n",
    "                Do not write any preamble or explanation other than asked in the task described.\n",
    "\n",
    "                ### Content to Get The Idea What This Image Is About:\n",
    "                {cleaned_content}\n",
    "\n",
    "                Generate a detailed description of the image. \n",
    "                Ensure that the description is comprehensive and no important data is missed.\n",
    "                ### Image Description:\"\"\"\n",
    "    \n",
    "    text_message = {\"type\": \"text\", \"text\": text}\n",
    "    \n",
    "    final_message = [text_message] + images_data\n",
    "\n",
    "    message = HumanMessage(content=final_message)\n",
    "\n",
    "    # ResponseError: vision model only supports a single image per message\n",
    "    # in case of Ollama Model, LLAMA3.2 Vision\n",
    "    # message = HumanMessage(content=[text_message, images_data[0]])               \n",
    "    \n",
    "    response = model.invoke([system_message, message])\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doclings \n",
    "- ðŸ—‚ï¸ Reads popular document formats (PDF, DOCX, PPTX, XLSX, Images, HTML, AsciiDoc & Markdown) and exports to HTML, Markdown and JSON (with embedded and referenced images)\n",
    "- ðŸ“‘ Advanced PDF document understanding including page layout, reading order & table structures\n",
    "- ðŸ§© Unified, expressive DoclingDocument representation format\n",
    "- ðŸ¤– Plug-and-play integrations incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI\n",
    "- ðŸ” OCR support for scanned PDFs\n",
    "- ðŸ’» Simple and convenient CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DS4SD/docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docling_parse.pdf_parsers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_docling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoclingLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_docling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExportType\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import Path\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\lib\\site-packages\\langchain_docling\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright IBM Corp. 2025 - 2025\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: MIT\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Docling LangChain package.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_docling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoclingLoader\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\lib\\site-packages\\langchain_docling\\loader.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChunk, BaseChunker, HybridChunker\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoclingDocument\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_converter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentConverter\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLoader\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\lib\\site-packages\\docling\\document_converter.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractDocumentBackend\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masciidoc_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsciiDocBackend\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocling_parse_v2_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoclingParseV2DocumentBackend\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLDocumentBackend\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkdownDocumentBackend\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\lib\\site-packages\\docling\\backend\\docling_parse_v2_backend.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdfium2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpdfium\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoundingBox, CoordOrigin\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling_parse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pdf_parser_v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpypdfium2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfPage\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docling_parse.pdf_parsers'"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "# import Path\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = r\"..\\00 Dataset\\docs\\facebook\\Meta-09-30-2024-Exhibit-99-1_FINAL.pdf\"\n",
    "FILE_PATH = r\"..\\00 Dataset\\docs\\facebook\\Downloadable-BS-Q3-24.xlsx\"\n",
    "# FILE_PATH = \"https://pdfobject.com/pdf/sample.pdf\"\n",
    "\n",
    "# get markdown of the files which don't have any figure\n",
    "def get_markdown_without_figure(input_doc_path, target_dir):\n",
    "    loader = DoclingLoader(file_path=input_doc_path,\n",
    "                           export_type=ExportType.MARKDOWN)\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    # doc file name\n",
    "    doc_filename = Path(input_doc_path).stem\n",
    "\n",
    "    md_filename =  f\"{target_dir}/{doc_filename}-with-image-refs.md\"\n",
    "\n",
    "    # write markdown to file\n",
    "    with open(md_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(docs[0].page_content)\n",
    "        f.close()\n",
    "\n",
    "    return md_filename\n",
    "\n",
    "\n",
    "input_doc_path = r\"..\\00 Dataset\\docs\\facebook\\Earnings-Presentation-Q3-2024.pdf\"\n",
    "md_filename = get_markdown_without_figure(input_doc_path, 'output')\n",
    "\n",
    "md_filename\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to MarkDown with Images and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docling_parse.pdf_parsers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageRefMode, PictureItem, TableItem\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfPipelineOptions\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_converter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentConverter, PdfFormatOption\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputFormat\n\u001b[0;32m      8\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\lib\\site-packages\\docling\\document_converter.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractDocumentBackend\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masciidoc_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsciiDocBackend\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocling_parse_v2_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DoclingParseV2DocumentBackend\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTMLDocumentBackend\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkdownDocumentBackend\n",
      "File \u001b[1;32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\lib\\site-packages\\docling\\backend\\docling_parse_v2_backend.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdfium2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpdfium\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoundingBox, CoordOrigin\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocling_parse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pdf_parser_v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpypdfium2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfPage\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docling_parse.pdf_parsers'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "def get_pdf_markdown(input_doc_path, target_dir):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(input_doc_path)  # Ensure it's a Path object\n",
    "    target_dir = Path(target_dir)  # Ensure it's a Path object\n",
    "    output_dir = Path(\"scratch\")  # Intermediate directory for storing images\n",
    "\n",
    "    # Configure the pipeline options\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    # Initialize the document converter\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert the input PDF document\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    # Ensure the output directories exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem\n",
    "\n",
    "    # Save page images\n",
    "    for page_no, page in conv_res.document.pages.items():\n",
    "        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "        with page_image_filename.open(\"wb\") as fp:\n",
    "            page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "    # Save images of figures and tables\n",
    "    table_counter = 0\n",
    "    picture_counter = 0\n",
    "    for element, _level in conv_res.document.iterate_items():\n",
    "        if isinstance(element, TableItem):\n",
    "            table_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "        if isinstance(element, PictureItem):\n",
    "            picture_counter += 1\n",
    "            element_image_filename = (\n",
    "                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "    # Save markdown with externally referenced pictures\n",
    "    md_filename = target_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "    return md_filename\n",
    "\n",
    "# # Usage example\n",
    "# input_doc_path = r\"..\\00 Dataset\\docs\\facebook\\Earnings-Presentation-Q3-2024.pdf\"\n",
    "# md_filename = get_pdf_markdown(input_doc_path, target_dir=\"output\")\n",
    "# print(md_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarkDown Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "# md_filename = r\"scratch\\Earnings-Presentation-Q3-2024-with-image-refs.md\"\n",
    "\n",
    "def get_markdown_splits(md_filename):\n",
    "    with open(md_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        markdown_content = f.read()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_content)\n",
    "\n",
    "    return md_header_splits\n",
    "\n",
    "md_header_splits = get_markdown_splits(md_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_urls_and_clean_content(page_content):\n",
    "    # Define the regex pattern to match image URLs\n",
    "    pattern = r\"!\\[Image\\]\\(([^)]+)\\)\"\n",
    "\n",
    "    # Find all matches in the page content\n",
    "    image_urls = re.findall(pattern, page_content)\n",
    "\n",
    "    # Remove all matched image URLs from the content\n",
    "    cleaned_content = re.sub(pattern, \"\", page_content)\n",
    "\n",
    "    return image_urls, cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000000_3db7e1eab3213fbdaaf83be4c917fbb9fd97bdf42a5033bd7e7e4df4b53f7b18.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000001_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000002_f3182f98eee364a6902eebd1c2cbf3ad411e606cf6d9a3e4fb0a07d2f3416e71.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000003_7a5af1bd15760003c9cd20a1f0ac84b7eb0585e9c23c8d401c7e4a3226887e25.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000004_424e40f9e79523cd2b70fb5344a886a7f916bf23ed25745431de3dcabb3a9134.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000005_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000006_dee327b9119ef6ee2940999dc13e68a8c93145e5d96b82e0461dfa7b71598c73.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000007_c7cb5242c17e44d54b0f9cc654d295579195c561d856168fe0663ee6710fdd06.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000008_dee327b9119ef6ee2940999dc13e68a8c93145e5d96b82e0461dfa7b71598c73.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000009_f85552109ed0a8cb32bf30d05db7be9394370217b99bb7fc069dc269f06d4e31.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000010_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000011_b6da5accc28d9d241fe64d0531855f295258e6eb478e83f551811f29b9cc2334.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000012_3dbd6ce9176178bbe19bdce419dec3d6aa2c7bdcc97a33ceca26515d3c6317bf.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000013_f2250e6474c3402323412b96a68692489027330905f02ff73358bf5ba9245edf.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000014_6541cfc816f64515a4fc272d7d9e768bee8742845b5641308e23e3f333ae86e2.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000015_b38533143e867d40a45b28680284201d641d46c933fd05b19a2f1b7b6933c0b4.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000016_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000017_278dad5e9c44f5552629e781407508746b2c8c43e8597f0a69705fe8f19aa660.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000018_a97f7ab741517b759207bb72675c7c3fb62ab12d825ed8bf27d15883e7a2a20d.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000019_d0331d45d03fb943b800e868ada67b63bec57d8f5dd8a8c145d11e9f9f95c293.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000020_a213f8d2d6e7a118e90be891e5d06561645b514797c7f596d6e9e1a32421981e.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000021_b879d51a8e176ee4e872be96173487040d33e3dd6d70a78b695fa698b7b13199.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000022_f5594792ca13a015a0338afcbe240930329eea556394c2c208c4ec4397b1a7d4.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000023_b30f9adff3ec6c5d729f060293ec45b08f5f6b846d4a0fd52bd8e32d50f3848d.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000024_dee327b9119ef6ee2940999dc13e68a8c93145e5d96b82e0461dfa7b71598c73.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000025_41715a006d333b722e233f87005e3dca2fc4acace1efa08db044b49b3db6447c.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000026_4f6a5573d20b3c2a38d819134250fa02bea2811cf29a59e9569845c27467d956.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000027_b6cc85d6ea3ebe02c87a01dae9b39805ed189fb8deebb1398ccbbc31ee516a6d.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000028_bc90987056a214c87c35b505e9157c0c9a1061768b16eea20ab9943fc428efb7.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000029_5e9251d796417c096e71a421be2b4be634a243736edae62cc1bcd690ece29c85.png', 'Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000030_bfc82afd96c94336e0fe98bc6950ac5a226e0826ca3f4ce488897cda70b65b6f.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000031_61331cf29cbe41e0cfd7585af316c8f4a55b1ab6ef528e0d2dfc9640cb8022a5.png']\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000032_fc1e32a9faebdcec75dc7acb7f32b5bc5fafd2b6681eac5f0e43abad585bd063.png']\n",
      "Image URLs: []\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000033_34a3fafa6be647cb65ed649fc2257054f06cba213a04887be094897b444badcf.png']\n",
      "Image URLs: []\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000034_34a3fafa6be647cb65ed649fc2257054f06cba213a04887be094897b444badcf.png']\n",
      "Image URLs: []\n",
      "Image URLs: ['Earnings-Presentation-Q3-2024-with-image-refs_artifacts%5Cimage_000035_1d0090d2a602b18b054d6d31be46f4fa66c6333f8b3e4ce0a1f93de3d7231ec8.png']\n"
     ]
    }
   ],
   "source": [
    "for page in md_header_splits:\n",
    "    # print(f\"Header: {page.metadata}\")\n",
    "    # print(f\"Content: {page.page_content}\")\n",
    "    # Extract image URLs from the page content\n",
    "    image_urls, cleaned_content = extract_image_urls_and_clean_content(page.page_content)\n",
    "    print(f\"Image URLs: {image_urls}\")\n",
    "    # print(f\"Cleaned Content: {cleaned_content}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich MarkDown with Images Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def enrich_document_with_image(md_header_splits):\n",
    "    documents = []\n",
    "    for page in md_header_splits:\n",
    "        image_urls, cleaned_content = extract_image_urls_and_clean_content(page.page_content)\n",
    "        # read image from file\n",
    "        \n",
    "        image_data_path = \"scratch\"\n",
    "        image_description = get_image_description(image_data_path, image_urls, cleaned_content)\n",
    "            \n",
    "        \n",
    "        merged_content = cleaned_content + \"\\n\\nExtracted Image Description:\\n\" + image_description\n",
    "\n",
    "        documents.append(merged_content)\n",
    "\n",
    "        # print(\"page: \", page.metadata)\n",
    "        # print(\"response: \", image_description)\n",
    "        # print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "    enriched_content = \"\\n\\n\".join(documents)\n",
    "\n",
    "    return enriched_content\n",
    "\n",
    "enriched_content = enrich_document_with_image(md_header_splits)\n",
    "\n",
    "with open(\"enriched_content.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(enriched_content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Entire Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "Could not load the custom kernel for multi-scale deformable attention: DLL load failed while importing MultiScaleDeformableAttention: The specified module could not be found.\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Earnings-Presentation-Q3-2024.pdf\n",
      "INFO:docling.document_converter:Finished converting document Earnings-Presentation-Q3-2024.pdf in 29.34 sec.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "input_doc_path = r\"..\\00 Dataset\\docs\\facebook\\Earnings-Presentation-Q3-2024.pdf\"\n",
    "md_filename = get_pdf_markdown(input_doc_path, 'output')\n",
    "\n",
    "md_header_splits = get_markdown_splits(md_filename)\n",
    "\n",
    "enriched_content = enrich_document_with_image(md_header_splits)\n",
    "\n",
    "with open(\"enriched_content.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(enriched_content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ..\\00 Dataset\\docs\\llm_paper.pdf\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_pdf_markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../00 Dataset/docs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../00 Dataset/markdown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mprocess_and_enrich_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mprocess_and_enrich_documents\u001b[1;34m(input_dir, output_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_pdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Step 1: Convert PDF to Markdown\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m md_filename \u001b[38;5;241m=\u001b[39m \u001b[43mget_pdf_markdown\u001b[49m(input_pdf_path, target_dir)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Step 2: Split the Markdown content (assume method available)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m md_header_splits \u001b[38;5;241m=\u001b[39m get_markdown_splits(md_filename)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_pdf_markdown' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_and_enrich_documents(input_dir, output_dir):\n",
    "\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        relative_path = Path(root).relative_to(input_dir)\n",
    "        target_dir = output_dir / relative_path\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)  # Ensure target directory exists\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.pdf', '.docx', '.pptx', '.xlsx')):\n",
    "                input_pdf_path = Path(root) / file\n",
    "                print(f\"Processing file: {input_pdf_path}\")\n",
    "                \n",
    "                # Step 1: Convert PDF to Markdown\n",
    "                md_filename = get_pdf_markdown(input_pdf_path, target_dir)\n",
    "                \n",
    "                # Step 2: Split the Markdown content (assume method available)\n",
    "                md_header_splits = get_markdown_splits(md_filename)\n",
    "                \n",
    "                # Step 3: Enrich the document (assume method available)\n",
    "                enriched_content = enrich_document_with_image(md_header_splits)\n",
    "                \n",
    "                # Step 4: Save enriched content\n",
    "                enriched_output_path = target_dir / f\"{Path(md_filename).stem}_enriched.md\"\n",
    "                with open(enriched_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(enriched_content)\n",
    "\n",
    "    print(f\"Processing complete. Enriched content saved to: {output_dir}\")\n",
    "\n",
    "# Example Usage\n",
    "input_dir = r\"../00 Dataset/docs\"\n",
    "output_dir = r\"../00 Dataset/markdown\"\n",
    "process_and_enrich_documents(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
